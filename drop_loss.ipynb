{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import Flatten, Dense, Activation, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# pre-trained weights require this ordering\n",
    "keras.backend.set_image_dim_ordering(\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # layer 1\n",
    "    model.add(Convolution2D(16, 3, 3, input_shape=(3, 448, 448), border_mode=\"same\", subsample=(1, 1)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode=\"valid\"))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode=\"valid\"))\n",
    "\n",
    "    # Layer 4\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode=\"valid\"))\n",
    "\n",
    "    # Layer 5\n",
    "    model.add(Convolution2D(256, 3, 3, border_mode=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode=\"valid\"))\n",
    "\n",
    "    # Layer 6\n",
    "    model.add(Convolution2D(512, 3, 3, border_mode=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode=\"valid\"))\n",
    "\n",
    "    # Layer 7\n",
    "    model.add(Convolution2D(1024, 3, 3, border_mode=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    # Layer 8\n",
    "    model.add(Convolution2D(1024, 3, 3, border_mode=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    # Layer 9\n",
    "    model.add(Convolution2D(1024, 3, 3, border_mode=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Layer 10\n",
    "    model.add(Dense(256))\n",
    "\n",
    "    # Layer 11\n",
    "    model.add(Dense(4096))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    # Layer 12\n",
    "    model.add(Dense(1470))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize(image):\n",
    "    PIL_image = Image.fromarray(image)\n",
    "    resized = PIL_image.resize((448, 448), Image.LANCZOS)\n",
    "    return np.array(resized)\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    normalized = 2. * image / 255. - 1.\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    resized = resize(image)\n",
    "    normalized = normalize(resized)\n",
    "    # model works on (channel, height, width) ordering of dimensions\n",
    "    transposed = np.transpose(normalized, (2, 0, 1))\n",
    "    return transposed\n",
    "\n",
    "\n",
    "class Box:\n",
    "    def __init__(self):\n",
    "        self.x, self.y = float(), float()\n",
    "        self.w, self.h = float(), float()\n",
    "        self.c = float()\n",
    "        self.prob = float()\n",
    "\n",
    "        \n",
    "def overlap(x1, w1, x2, w2):\n",
    "    l1 = x1 - w1 / 2.\n",
    "    l2 = x2 - w2 / 2.\n",
    "    left = max(l1, l2)\n",
    "    r1 = x1 + w1 / 2.\n",
    "    r2 = x2 + w2 / 2.\n",
    "    right = min(r1, r2)\n",
    "    return right - left\n",
    "\n",
    "\n",
    "# return intersection of box a and box b\n",
    "def box_intersection(a, b):\n",
    "    w = overlap(a.x, a.w, b.x, b.w)\n",
    "    h = overlap(a.y, a.h, b.y, b.h)\n",
    "    if w < 0 or h < 0:\n",
    "        return 0\n",
    "    area = w * h\n",
    "    return area\n",
    "\n",
    "\n",
    "# return area under union of box a and box b\n",
    "def box_union(a, b):\n",
    "    i = box_intersection(a, b)\n",
    "    u = a.w * a.h + b.w * b.h - i\n",
    "    return u\n",
    "\n",
    "\n",
    "# return intersection over union of box a and box b\n",
    "def box_iou(a, b):\n",
    "    return box_intersection(a, b) / box_union(a, b)\n",
    "\n",
    "\n",
    "def yolo_output_to_car_boxes(yolo_output, threshold=0.2, sqrt=1.8, C=20, B=2, S=7):\n",
    "    # position for class car in the VOC dataset classes\n",
    "    car_class_number = 6\n",
    "\n",
    "    boxes = []\n",
    "    SS = S * S  # number of grid cells\n",
    "    prob_size = SS * C  # class probabilities\n",
    "    conf_size = SS * B  # confidences for each grid cell\n",
    "\n",
    "    probabilities = yolo_output[0:prob_size]\n",
    "    confidence_scores = yolo_output[prob_size: (prob_size + conf_size)]\n",
    "    cords = yolo_output[(prob_size + conf_size):]\n",
    "\n",
    "    # reshape the arrays so that its easier to loop over them\n",
    "    probabilities = probabilities.reshape((SS, C))\n",
    "    confs = confidence_scores.reshape((SS, B))\n",
    "    cords = cords.reshape((SS, B, 4))\n",
    "\n",
    "    for grid in range(SS):\n",
    "        for b in range(B):\n",
    "            bx = Box()\n",
    "\n",
    "            bx.c = confs[grid, b]\n",
    "\n",
    "            # bounding box xand y coordinates are offsets of a particular grid cell location,\n",
    "            # so they are also bounded between 0 and 1.\n",
    "            # convert them absolute locations relative to the image size\n",
    "            bx.x = (cords[grid, b, 0] + grid % S) / S\n",
    "            bx.y = (cords[grid, b, 1] + grid // S) / S\n",
    "\n",
    "\n",
    "            bx.w = cords[grid, b, 2] ** sqrt\n",
    "            bx.h = cords[grid, b, 3] ** sqrt\n",
    "\n",
    "            # multiply confidence scores with class probabilities to get class sepcific confidence scores\n",
    "            p = probabilities[grid, :] * bx.c\n",
    "\n",
    "            # Check if the confidence score for class car is greater than the threshold\n",
    "            if p[car_class_number] >= threshold:\n",
    "                bx.prob = p[car_class_number]\n",
    "                boxes.append(bx)\n",
    "\n",
    "    # combine boxes that overlap\n",
    "    boxes.sort(key=lambda b: b.prob, reverse=True)\n",
    "    for i in range(len(boxes)):\n",
    "        boxi = boxes[i]\n",
    "        if boxi.prob == 0:\n",
    "            continue\n",
    "\n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            boxj = boxes[j]\n",
    "\n",
    "            # if boxes have more than 40% overlap then retain the box with the highest confidence score\n",
    "            if box_iou(boxi, boxj) >= 0.4:\n",
    "                boxes[j].prob = 0\n",
    "\n",
    "    boxes = [b for b in boxes if b.prob > 0]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def draw_boxes(boxes, im, crop_dim):\n",
    "    PIL_image = Image.fromarray(im.copy())\n",
    "    draw = ImageDraw.Draw(PIL_image)\n",
    "    [xmin, xmax] = crop_dim[0]\n",
    "    [ymin, ymax] = crop_dim[1]\n",
    "    \n",
    "    height, width, _ = im.shape\n",
    "    for b in boxes:\n",
    "        w = xmax - xmin\n",
    "        h = ymax - ymin\n",
    "\n",
    "        left = int((b.x - b.w / 2.) * w) + xmin\n",
    "        right = int((b.x + b.w / 2.) * w) + xmin\n",
    "        top = int((b.y - b.h / 2.) * h) + ymin\n",
    "        bot = int((b.y + b.h / 2.) * h) + ymin\n",
    "\n",
    "        if left < 0:\n",
    "            left = 0\n",
    "        if right > width - 1:\n",
    "            right = width - 1\n",
    "        if top < 0:\n",
    "            top = 0\n",
    "        if bot > height - 1: \n",
    "            bot = height - 1\n",
    "        \n",
    "        thickness = 3\n",
    "        draw.line([(left, bot), (left, top)], fill=(255, 0, 0), width=thickness)\n",
    "        draw.line([(left, top), (right, top)], fill=(255, 0, 0), width=thickness)\n",
    "        draw.line([(right, top), (right, bot)], fill=(255, 0, 0), width=thickness)\n",
    "        draw.line([(right, bot), (left, bot)], fill=(255, 0, 0), width=thickness)\n",
    "\n",
    "    del draw\n",
    "    return np.array(PIL_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weights(model, yolo_weight_file):\n",
    "    data = np.fromfile(yolo_weight_file, np.float32)\n",
    "    data = data[4:]\n",
    "\n",
    "    index = 0\n",
    "    for layer in model.layers:\n",
    "        shape = [w.shape for w in layer.get_weights()]\n",
    "        if shape != []:\n",
    "            kshape, bshape = shape\n",
    "            bia = data[index:index + np.prod(bshape)].reshape(bshape)\n",
    "            index += np.prod(bshape)\n",
    "            ker = data[index:index + np.prod(kshape)].reshape(kshape)\n",
    "            index += np.prod(kshape)\n",
    "            layer.set_weights([ker, bia])\n",
    "            \n",
    "\n",
    "model = get_model()\n",
    "load_weights(model, \"../weights/yolo-tiny.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = Image.open(\"../dataset/videos_drop_loss/1_30pct_drop/image_010.png\")\n",
    "image = image.convert(\"RGB\")\n",
    "image = np.array(image)\n",
    "preprocessed = preprocess(image)\n",
    "batch = np.expand_dims(preprocessed, axis=0)\n",
    "batch_output = model.predict(batch)\n",
    "boxes = yolo_output_to_car_boxes(batch_output[0], threshold=0.20)\n",
    "final = draw_boxes(boxes, image, ((0, image.shape[1]),(0, image.shape[0])))\n",
    "\n",
    "plt.imshow(final)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop loss evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annot_dict = pickle.load(open(\"../dataset/annot_dict.p\", \"rb\"))\n",
    "dir_prefix = \"../dataset/videos_drop_loss\"\n",
    "drop_rates = [\"10pct\", \"20pct\", \"30pct\"]\n",
    "video_names = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "\n",
    "annot_dir_names = [\"../dataset/1_annot\", \"../dataset/2_annot\", \"../dataset/3_annot\",\n",
    "                   \"../dataset/4_annot\", \"../dataset/5_annot\", \"../dataset/6_annot\"]\n",
    "col_names = [\"Video name\", \"Image no. (p=0.1)\", \"IoU (p=0.1)\", \"Conf. (p=0.1)\", \"Prob. (p=0.1)\", \"Image no. (p=0.2)\",\n",
    "             \"IoU (p=0.2)\", \"Conf. (p=0.2)\", \"Prob. (p=0.2)\", \"Image no. (p=0.3)\", \"IoU (p=0.3)\", \"Conf. (p=0.3)\", \n",
    "             \"Prob. (p=0.3)\"]\n",
    "df = pd.DataFrame(index=range(len(video_names)), columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kept_frames_df = pd.read_csv(os.path.join(dir_prefix, \"frames_kept.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "for k in range(0, len(video_names)):\n",
    "    video_name = video_names[k]\n",
    "    annot_dir = annot_dir_names[k]\n",
    "\n",
    "    for drop_rate in drop_rates:\n",
    "        dir_name = os.path.join(dir_prefix, video_name + \"_\" + drop_rate + \"_drop\")\n",
    "        \n",
    "        num_images = 0\n",
    "        for f in os.listdir(dir_name):\n",
    "            if f.endswith(\".png\") and not f.startswith(\".\"):\n",
    "                num_images += 1\n",
    "\n",
    "        print \"Starting \" + dir_name\n",
    "        print\n",
    "        \n",
    "        mean_iou_all = []\n",
    "        c_all = []\n",
    "        prob_all = []\n",
    "        results_dict[dir_name] = {}\n",
    "        \n",
    "        kept_frames = kept_frames_df[dir_name.split(\"/\")[-1]].dropna().tolist()\n",
    "\n",
    "        for i in range(1, num_images + 1):\n",
    "            # make predictions\n",
    "            image_name = \"image_\" + format(i, \"03d\") + \".png\"\n",
    "            image_file = os.path.join(dir_name, image_name)\n",
    "            image = Image.open(image_file)\n",
    "            image = image.convert(\"RGB\")\n",
    "            image = np.array(image)\n",
    "            preprocessed = preprocess(image)\n",
    "            batch = np.expand_dims(preprocessed, axis=0)\n",
    "            batch_output = model.predict(batch)\n",
    "            boxes = yolo_output_to_car_boxes(batch_output[0], threshold=0.20)\n",
    "\n",
    "            # get annotations and compare\n",
    "            annot_image_name = \"image_\" + format(int(kept_frames[i - 1]) + 1, \"03d\") + \".png\"\n",
    "            annot_boxes = annot_dict[annot_dir][annot_image_name]\n",
    "            num_annot_boxes = len(annot_boxes)\n",
    "            total_iou = 0.0\n",
    "            for annot_box in annot_boxes:\n",
    "                best_iou = 0.0\n",
    "                for b in boxes:\n",
    "                    curr_iou = box_iou(annot_box, b)\n",
    "                    if curr_iou > best_iou:\n",
    "                        best_iou = curr_iou\n",
    "                total_iou += best_iou\n",
    "            mean_iou = total_iou / num_annot_boxes\n",
    "            mean_iou_all.append(mean_iou)\n",
    "\n",
    "            confidences = []\n",
    "            probabilities = []\n",
    "            for b in boxes:\n",
    "                confidences.append(b.c)\n",
    "                probabilities.append(b.prob)\n",
    "\n",
    "            if len(confidences) == 0:\n",
    "                c_mean = 0.0\n",
    "            else:\n",
    "                c_mean = np.mean(confidences)\n",
    "            if len(probabilities) == 0:\n",
    "                prob_mean = 0.0\n",
    "            else:\n",
    "                prob_mean = np.mean(probabilities)\n",
    "\n",
    "            c_all.append(c_mean)\n",
    "            prob_all.append(prob_mean)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(\"Mean IoU: \" + str(mean_iou))\n",
    "                print(\"Mean confidence: \" + str(c_mean))\n",
    "                print(\"Mean probability: \" + str(prob_mean))\n",
    "                print \"Done with image \" + str(i) + \" of \" + str(num_images)\n",
    "                print\n",
    "\n",
    "        results_dict[dir_name][\"iou\"] = mean_iou_all\n",
    "        results_dict[dir_name][\"c\"] = c_all\n",
    "        results_dict[dir_name][\"prob\"] = prob_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, video_name in enumerate(video_names):\n",
    "    df.iloc[i, 0] = video_name\n",
    "    \n",
    "    dir_name = os.path.join(dir_prefix, video_name + \"_10pct_drop\")\n",
    "    df.iloc[i, 1] = len(results_dict[dir_name][\"iou\"])\n",
    "    df.iloc[i, 2] = np.mean(results_dict[dir_name][\"iou\"])\n",
    "    df.iloc[i, 3] = np.mean(results_dict[dir_name][\"c\"])\n",
    "    df.iloc[i, 4] = np.mean(results_dict[dir_name][\"prob\"])\n",
    "    \n",
    "    dir_name = os.path.join(dir_prefix, video_name + \"_20pct_drop\")\n",
    "    df.iloc[i, 5] = len(results_dict[dir_name][\"iou\"])\n",
    "    df.iloc[i, 6] = np.mean(results_dict[dir_name][\"iou\"])\n",
    "    df.iloc[i, 7] = np.mean(results_dict[dir_name][\"c\"])\n",
    "    df.iloc[i, 8] = np.mean(results_dict[dir_name][\"prob\"])\n",
    "    \n",
    "    dir_name = os.path.join(dir_prefix, video_name + \"_30pct_drop\")\n",
    "    df.iloc[i, 9] = len(results_dict[dir_name][\"iou\"])\n",
    "    df.iloc[i, 10] = np.mean(results_dict[dir_name][\"iou\"])\n",
    "    df.iloc[i, 11] = np.mean(results_dict[dir_name][\"c\"])\n",
    "    df.iloc[i, 12] = np.mean(results_dict[dir_name][\"prob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"../results/drop_loss_df.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(results_dict, open(\"../results/drop_loss_results.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
